Benchmark,MixtureVitae,Comma-0.1,CommonCorpus (eng),FineWeb-Edu,DCLM
COPA,0.73,0.71,0.71,0.76,0.81
LAMBADA,0.4799146128468853,0.5441490393945274,0.4909761304094702,0.5171744614787502,0.6547642150203765
OpenBookQA,0.354,0.328,0.31,0.416,0.392
Winogrande,0.5816890292028414,0.5951065509076559,0.5619573796369376,0.6069455406471981,0.6243093922651933
MMLU,0.3762996724113374,0.2652043868394815,0.2508189716564591,0.2618572852869961,0.2506765418031619
ARC_Challenge,0.3950511945392491,0.3626279863481229,0.3233788395904436,0.439419795221843,0.3967576791808874
ARC_Easy,0.7117003367003367,0.6313131313131313,0.6119528619528619,0.7462121212121212,0.7297979797979798
BoolQ,0.7516819571865443,0.618348623853211,0.6165137614678899,0.6718654434250765,0.691131498470948
CommonsenseQA,0.4905814905814906,0.2129402129402129,0.1932841932841932,0.1941031941031941,0.2006552006552006
HellaSwag,0.5385381398127863,0.5344552877912766,0.4520015933081059,0.6266679944234216,0.6682931686914957
PIQA,0.6985854189336235,0.7116430903155604,0.6588683351468988,0.7557127312295974,0.7611534276387377
Average,0.555276532019554,0.5012534827002889,0.4708865514957509,0.5450871424571089,0.5617762821385437
